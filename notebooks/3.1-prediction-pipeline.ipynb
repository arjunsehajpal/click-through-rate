{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this notebook is to convert the `data-aggregation.ipynb` code to Spark, so, we can use it in deployment and eliminate the memory errors that we usually encounter while working on the pandas. We will use Spark's Python API, *PySpark* for this purpose. The benefit of using PySpark is that it provides the data manipulation and Analytics abilities along with support of Machine Learning. PySpark is very easy to use for anyone who has prior experience in Python or SQL. \n",
    "\n",
    "This code specifically caters to the Training pipeline. The following are the operations that will be performed on the data in the training cycle:-\n",
    "1. Converting the date columns to datetime format (Done)\n",
    "2. Calculate mean ``impression_time`` for a particular user\n",
    "3. Encode ``os_version``\n",
    "4. Count unique apps used by the user\n",
    "5. Extract hour and minute from ``impression_time``\n",
    "6. Count unique user for an app\n",
    "7. Calculating the counts of ``user_id`` and ``app_code``\n",
    "8. Checking how many times user has clicked the ad and what was the last time he had an impression\n",
    "9. Checking how many times user has clicked the ad from  A PARTICULAR APP and what was the last time he had an impression\n",
    "10. In `view_aggdf`, encode the device type\n",
    "11. merge `view_aggdf` and `train_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the libraries\n",
    "import os\n",
    "import pickle\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version of Spark = 2.4.5\n"
     ]
    }
   ],
   "source": [
    "# setting up spark\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"PySpark_feature_eng\")\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# set sqlContext from the Spark context\n",
    "sqlContext = SQLContext(spark)\n",
    "\n",
    "# checking the version of Spark\n",
    "print(\"Version of Spark = {}\".format(spark.version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup a spark SQL context and read in the pandas df to a spark df\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(root_dir, verbose = False):\n",
    "    \"\"\"\n",
    "    fetches the data using pandas and converts them into spark dataframes\n",
    "    \n",
    "    inp: path of root directory\n",
    "    returns: spark dataframe \n",
    "    \"\"\"\n",
    "    \n",
    "    def convert_to_string(df):\n",
    "        \"\"\"\n",
    "        inner function that converts dtype of columns to string\n",
    "        \"\"\"\n",
    "        for i in list(df.columns):\n",
    "            df[i] = df[i].astype(\"str\")\n",
    "\n",
    "        return df\n",
    "    \n",
    "    # defining the paths\n",
    "    trainpath = os.path.join(root_dir, \"data\", \"train.csv\")\n",
    "    item_data_path = os.path.join(root_dir, \"data\", \"item_data.csv\")\n",
    "    view_log_path = os.path.join(root_dir, \"data\", \"view_log.csv\")\n",
    "    testpath = os.path.join(root_dir, \"data\", \"test.csv\")\n",
    "\n",
    "    # importing the datasets\n",
    "    train_df = pd.read_csv(trainpath)\n",
    "    item_df = pd.read_csv(item_data_path)\n",
    "    view_df = pd.read_csv(view_log_path)\n",
    "    test_df = pd.read_csv(testpath)\n",
    "    \n",
    "    if verbose:\n",
    "        logging.info(\"Datasets read by pandas\")\n",
    "    \n",
    "    # converting columns' dtypes to string\n",
    "    train_df = convert_to_string(train_df)\n",
    "    item_df = convert_to_string(item_df)\n",
    "    view_df = convert_to_string(view_df)\n",
    "    test_df = convert_to_string(test_df)\n",
    "    \n",
    "#     # setup a spark SQL context and read in the pandas df to a spark df\n",
    "#     spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "\n",
    "    # converting to spark df\n",
    "    train_df = sqlContext.createDataFrame(train_df)\n",
    "    item_df = sqlContext.createDataFrame(item_df)\n",
    "    view_df = sqlContext.createDataFrame(view_df)\n",
    "    test_df = sqlContext.createDataFrame(test_df)\n",
    "    \n",
    "    # Now that we have converted all pandas df to spark df, we will correct all the column types.\n",
    "    # for train_df\n",
    "    train_df = train_df.withColumn(\"impression_id\", train_df[\"impression_id\"].cast(StringType()))\n",
    "    train_df = train_df.withColumn(\"impression_time\", train_df[\"impression_time\"].cast(TimestampType()))\n",
    "    train_df = train_df.withColumn(\"user_id\", train_df[\"user_id\"].cast(StringType()))\n",
    "    train_df = train_df.withColumn(\"app_code\", train_df[\"app_code\"].cast(StringType()))\n",
    "    train_df = train_df.withColumn(\"os_version\", train_df[\"os_version\"].cast(StringType()))\n",
    "    train_df = train_df.withColumn(\"is_4g\", train_df[\"is_4g\"].cast(ByteType()))\n",
    "    train_df = train_df.withColumn(\"is_click\", train_df[\"is_click\"].cast(ByteType()))\n",
    "\n",
    "    # for item_df\n",
    "    item_df = item_df.withColumn(\"item_id\", item_df[\"item_id\"].cast(StringType()))\n",
    "    item_df = item_df.withColumn(\"item_price\", item_df[\"item_price\"].cast(FloatType()))\n",
    "    item_df = item_df.withColumn(\"category_1\", item_df[\"category_1\"].cast(StringType()))\n",
    "    item_df = item_df.withColumn(\"category_2\", item_df[\"category_2\"].cast(StringType()))\n",
    "    item_df = item_df.withColumn(\"category_3\", item_df[\"category_3\"].cast(StringType()))\n",
    "    item_df = item_df.withColumn(\"product_type\", item_df[\"product_type\"].cast(StringType()))\n",
    "\n",
    "    # for view_df\n",
    "    view_df = view_df.withColumn(\"server_time\", view_df[\"server_time\"].cast(TimestampType()))\n",
    "    view_df = view_df.withColumn(\"device_type\", view_df[\"device_type\"].cast(StringType()))\n",
    "    view_df = view_df.withColumn(\"session_id\", view_df[\"session_id\"].cast(StringType()))\n",
    "    view_df = view_df.withColumn(\"user_id\", view_df[\"user_id\"].cast(StringType()))\n",
    "    view_df = view_df.withColumn(\"item_id\", view_df[\"item_id\"].cast(StringType()))\n",
    "    \n",
    "    # sorting the values by timestamps\n",
    "    view_df = view_df.orderBy(view_df[\"server_time\"], ascending = True)\n",
    "    train_df = train_df.orderBy(train_df[\"impression_time\"], ascending = True)\n",
    "    \n",
    "    if verbose:\n",
    "        logging.info(\"Datasets converted to Spark Dataframes\")\n",
    "    \n",
    "    \n",
    "    return train_df, item_df, view_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Datasets read by pandas\n",
      "INFO:root:Datasets converted to Spark Dataframes\n"
     ]
    }
   ],
   "source": [
    "# setting up root directory\n",
    "root_dir = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "\n",
    "# reading the data\n",
    "train_df, item_df, view_df = get_data(root_dir, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.group.GroupedData at 0x7fa366b269b0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.groupby(\"user_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
